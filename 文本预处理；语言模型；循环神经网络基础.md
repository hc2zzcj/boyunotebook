> 伯禹 文本预处理课程学习

#### 文本预处理

一般的文本预处理步骤：

1. 分词（中英文不同，中文分词难度大一些，有一些分词工具：spaCy、NLTK、jieba等，还可以在分词之后去除停用词等，根据语料及使用场景决定）
2. 词的向量表示（One-hot（资料中的词典），Word2Vec，可以参考https://www.cnblogs.com/MartinLwx/p/10005520.html。

#### 语言模型

文本可以看做一个词的序列，语言模型的目标就是评估该序列是否合理，也就是条件概率$P(w_1,w_2,\dots,w_T)$来表示文本是否合理。

$P(w_1,w_2,\dots,w_T)=\prod_{t=1}^TP(w_T|w_1,\dots,w_{t-1})=P(w_1)P(w_2|w_1)\dots P(w_T|w_1w_2\dotsw_{T-1})$

w为一个单词，P的计算可以用相对词频计算：

$P(w_1)=\frac {n(w_1)}n$

$P(w_1|w_2)=\frac {n(w_1,w_2)}{n(w_1)}$

##### n元语法（n-gram）

当前单词的预测基于前面n个单词，例如当n=2时：

$P(w_1,w_2,w_3,w_4)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_3)=P(w_1)P(w_2|w_1)P(w_3|w_2)p(w_4|w_3)$

##### 时序数据的采样问题

n>2时，n元语法存在大量重合样本。可参考如下：文本“想要有直升机，想要和你飞到宇宙去”，n=5，可能存在的样本有

+ X:"想要有直升",Y:"要有直升机"
+ X:"要有直升机",Y:"有直升机，"
+ $\dots$
+ X:“你飞到宇宙”,Y:"飞到宇宙去"

可以采用更加高效的采样方式：随机采样，相邻采样。这两种采样中不会再存在重合样本，简单来说相邻采样中两个相邻的batch原始位置相邻，随机采样中两个相邻的batch原始位置不一定相邻