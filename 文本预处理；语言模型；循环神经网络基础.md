> 伯禹 文本预处理课程学习

#### 文本预处理

一般的文本预处理步骤：

1. 分词（中英文不同，中文分词难度大一些，有一些分词工具：spaCy、NLTK、jieba等，还可以在分词之后去除停用词等，根据语料及使用场景决定）
2. 词的向量表示（One-hot（资料中的词典），Word2Vec，可以参考https://www.cnblogs.com/MartinLwx/p/10005520.html。

#### 语言模型

文本可以看做一个词的序列，语言模型的目标就是评估该序列是否合理，也就是条件概率$P(w_1,w_2,\dots,w_T)$来表示文本是否合理。

$P(w_1,w_2,\dots,w_T)=\prod_{t=1}^TP(w_T|w_1,\dots,w_{t-1})=P(w_1)P(w_2|w_1)\dots P(w_T|w_1w_2\dotsw_{T-1})$

w为一个单词，P的计算可以用相对词频计算：

$P(w_1)=\frac {n(w_1)}n$

$P(w_1|w_2)=\frac {n(w_1,w_2)}{n(w_1)}$

##### n元语法（n-gram）

当前单词的预测基于前面n个单词，例如当n=2时：

$P(w_1,w_2,w_3,w_4)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_3)=P(w_1)P(w_2|w_1)P(w_3|w_2)p(w_4|w_3)$

##### 时序数据的采样问题

n>2时，n元语法存在大量重合样本。可参考如下：文本“想要有直升机，想要和你飞到宇宙去”，n=5，可能存在的样本有

+ X:"想要有直升",Y:"要有直升机"
+ X:"要有直升机",Y:"有直升机，"
+ $\dots$
+ X:“你飞到宇宙”,Y:"飞到宇宙去"

可以采用更加高效的采样方式：随机采样，相邻采样。这两种采样中不会再存在重合样本，简单来说相邻采样中两个相邻的batch原始位置相邻，随机采样中两个相邻的batch原始位置不一定相邻

#### RNN基础

![rnn](D:\study\课程\rnn.png)

torch.gather()与torch.sactter_()是一对作用相反的方法，可以参考：https://blog.csdn.net/Teeyohuang/article/details/82186666

用torch.scatter_()实现onehot极其方便..

```  result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1
def one_hot(x, n_class, dtype=torch.float32):
    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)  # shape: (n, n_class)
    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1
    return result
  
def to_onehot(X, n_class):
    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]

X = torch.arange(10).view(2, 5)
inputs = to_onehot(X, vocab_size)
print(len(inputs), inputs[0].shape)
```

这里X=[[0,1,2,3,4],[5,6,7,8,9]]，batchsize=2，n=5

关于rnn的实现：

```
def rnn(inputs, state, params):
    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)
        Y = torch.matmul(H, W_hq) + b_q
        outputs.append(Y)
    return outputs, (H,)
```

可以看出rnn的设计中循环使用一个神经网络，参数重复使用，输入是一个序列，输出也是一个序列。

#### 课后练习错题：

给定训练数据[0,1,2,3,4,5,6,7,8,9,10]，批量大小为2，时间步数为2，使用相邻采样，第二个批量为：

1. [5,6] [7,8]
2. [2,3] [7,8]    正确答案
3. [4,5] [6,7]
4. [2,3] [6,7]     错误答案

答案解释：

> 参考视频15分30秒起对于相邻采样的解释。因为训练数据中总共有11个样本，而批量大小为2，所以数据集会被拆分成2段，每段包含5个样本：`[0, 1, 2, 3, 4]`和`[5, 6, 7, 8, 9]`，而时间步数为2，所以第二个批量为`[2, 3]`和`[7, 8]`。

看一下代码实现：

```
def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度  !!!在这一步处理结果为[0,1,2,3,4,5,6,7,8,9]
    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符
    indices = torch.tensor(corpus_indices, device=device)
    indices = indices.view(batch_size, -1)  # resize成(batch_size, )
    !!! 这一步处理结果为:[[0,1,2,3,4],[5,6,7,8,9]],后面的就很好理解
    batch_num = (indices.shape[1] - 1) // num_steps
    for i in range(batch_num):
        i = i * num_steps
        X = indices[:, i: i + num_steps]
        Y = indices[:, i + 1: i + num_steps + 1]
        yield X, Y
```

