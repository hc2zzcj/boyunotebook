#### 训练、泛化误差/训练集测试集验证集

训练误差 training error

泛化误差  generalization error

训练集 train set

测试集 test set

验证集 valid set

训练误差也就是模型在训练数据集上表现出的误差，泛化误差可以是模型在任意一个数据集上的误差，包括测试集。由于测试集只能在所有超参和模型参数确定以后使用一次，不能使用测试数据选择模型，然后又由于训练误差和泛化误差的关系，也不应该只依赖训练数据选择模型，我们选择预留一部分在训练集与测试集之外的数据来进行模型选择。即验证集。很多框架都提供了K折交叉验证。

#### 过拟合 欠拟合

在训练一个模型的过程中，有很多超参需要我们调节，最简单的epoch，learning rate，等等。主要就依靠个人经验以及当前模型的表现。有两类经典问题:

+ 模型无法得到较低的训练误差--->欠拟合（underfitting）
+ 模型的训练误差远远小于测试误差--->过拟合(overfitting)

![欠拟合](D:\study\课程\欠拟合_模型复杂度.png)

从一般的模型训练loss看，欠拟合，过拟合

![overfitting](D:\study\课程\overfitting.png)

![train_loss](D:\study\课程\train_loss.png)

![underfitting](D:\study\课程\underfitting.png)

#### 解决方案

一般欠拟合，可以降低模型复杂度或者增加数据集

过拟合的解决有很多，drouout，regularization等

##### L2范数正则化（regularization)

在损失函数上添加L2范数惩罚项，期望得到训练所需的最小化的函数

##### dropout

在训练过程中，以概率p丢弃神经元的数值(归零)

#### 梯度消失和梯度爆炸

如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。

Pytorch中nn.Module 模块参数都采取了较为合理的初始化策略

#### kaggle 房价预测

pandas数据类型：

+ 字符串类型：object
+ 整数类型：Int64, Int32, Int16, Int8
+ 无符号整数：UInt64, UInt32, UInt16,UInt8
+ 浮点数类型：float64, float32
+ 日期和时间类型：datatime64[ns], datatime64[ns, tz], timedelta[ns]
+ 布尔类型：bool

##### 模型选择

考虑到数据尺寸，直接用多层感知机，找一个合适的中间层神经元数量。

```
class Net(nn.Module):
    def __init__(self,hidden):
        super(Net, self).__init__()
        self.linear1 = nn.Linear(331, hidden)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(hidden, 1)
    
    def forward(self,x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        
        return x
```

考虑到数据量有限，从10-20中间找可能比较合适

![hid_10](D:\study\课程\hid_10.png)

![hid_14](D:\study\课程\hid_14.png)

![hid_18](D:\study\课程\hid_18.png)

![hid_20](D:\study\课程\hid_20.png)

上面4图分别为隐藏层参数为：10/14/18/20

后面试了一下完整的训练。隐藏层节点数为10train rmse为0.18,16 train rmse为0.13



















